{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Stanza parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "# stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse') # initialize English neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [\n",
    "        # 'he sleeps.',\n",
    "        # 'he sleeps his opponent in the tournament.', # Develope new slots\n",
    "\n",
    "        # 'he is a big bad wolf', # Multiple similar slots of one target word\n",
    "        # 'he runs the company and the firm' # Multiple similar slots of one target word\n",
    "\n",
    "        # 'This is Microsoft Windows operating system.', # Changing POS from N -> ProN\n",
    "        # 'This is the doors and the windows',\n",
    "        # 'Windows cannot compete against MacOS',\n",
    "\n",
    "        'This is a gay day.',\n",
    "        'I am a gay!',\n",
    "        'You are such a gay mofo!!!'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sents:\n",
    "    doc = nlp(sent) # run annotation over a sentence\n",
    "    print('sentence:', sent)\n",
    "    # print(doc)\n",
    "    # print(doc.entities)\n",
    "    print(*[f'word: {word.text}\\tlemma: {word.lemma}\\tpos: {word.pos}\\tid: {word.id}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "    print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "pattern = re.compile(\n",
    "    r'([^\\t]+)\\t'      # word form\n",
    "    r'([^\\t]+)\\t'      # lemma\n",
    "    r'([^\\t])[^\\t]*\\t' # POS (UPOS or XPOS)\n",
    "    r'([^\\t]+)\\t'      # ID\n",
    "    r'([^\\t]+)\\t'      # HEAD\n",
    "    r'([^\\t]+)'        # DEPREL\n",
    ")\n",
    "\n",
    "target_lemma = 'air'\n",
    "target_pos = 'N'\n",
    "\n",
    "# All\n",
    "period = '1750-1799'\n",
    "corpus_folder = f'/home/volt/bach/pilot_data/RSC/1750-1799_che_half_decades'\n",
    "output_folder = f'/home/volt/bach/SynFlow/output/{target_lemma}-{target_pos}-{period}'\n",
    "output_explorer = f'{output_folder}/Explorer'\n",
    "output_embedding = f'{output_folder}/Embedding'\n",
    "\n",
    "if not os.path.exists(output_explorer):\n",
    "    os.makedirs(output_explorer)\n",
    "\n",
    "if not os.path.exists(output_embedding):\n",
    "    os.makedirs(output_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the distribution of different syntactic relationships from the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynFlow.Explorer import arg_explorer\n",
    "dist = arg_explorer(\n",
    "    corpus_folder=corpus_folder,\n",
    "    target_lemma=target_lemma,\n",
    "    target_pos=target_pos,\n",
    "    max_length=1,\n",
    "    top_n=30,\n",
    "    pattern=pattern,\n",
    "    output_folder=output_explorer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynFlow.Explorer import arg_explorer\n",
    "dist = arg_explorer(\n",
    "    corpus_folder=corpus_folder,\n",
    "    target_lemma=target_lemma,\n",
    "    target_pos=target_pos,\n",
    "    max_length=2,\n",
    "    top_n=30,\n",
    "    pattern=pattern,\n",
    "    output_folder=output_explorer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Argument Combination Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 unique full-pattern string for 1 token\n",
    "from SynFlow.Explorer import arg_comb_explorer\n",
    "\n",
    "ctr = arg_comb_explorer(\n",
    "    corpus_folder=corpus_folder,\n",
    "    target_lemma=target_lemma,\n",
    "    target_pos=target_pos,\n",
    "    max_length=1,\n",
    "    top_n=30,\n",
    "    output_folder=output_explorer,\n",
    "    pattern=pattern\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rel Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynFlow.Explorer import rel_explorer\n",
    "\n",
    "rel_explorer_results = rel_explorer(\n",
    "    corpus_folder=corpus_folder,\n",
    "    pattern=pattern,            # or leave None to use default\n",
    "    target_lemma=target_lemma,\n",
    "    target_pos=target_pos,\n",
    "    rel=\"chi_amod\",\n",
    ")\n",
    "\n",
    "# inspect a few\n",
    "# for sent, ctx_list, path in triples[:10]:\n",
    "#     print(f\"{path:>15}  {' > '.join(ctx_list):20} | {sent}\")\n",
    "\n",
    "for fname, sent, ctx_list, path in rel_explorer_results[:1000]:\n",
    "    # ctx_list là một list các \"lemma/pos\", nối bằng ' > ' để in cho dễ nhìn\n",
    "    ctx_chain = \" > \".join(ctx_list)\n",
    "    print(f\"{fname:20} | {path:15} | {ctx_chain:20} | {sent}\")\n",
    "\n",
    "# Save to csv\n",
    "import csv\n",
    "out_path = f'{output_explorer}/rel_explorer.csv'\n",
    "with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    w = csv.writer(f, delimiter='\\t')\n",
    "    w.writerow([\"file\", \"path\", \"ctx_nodes\", \"sentence\"])\n",
    "    for fname, sent, ctx_nodes, path in rel_explorer_results:\n",
    "        ctx_chain = \" > \".join(ctx_nodes)\n",
    "        w.writerow([fname, path, ctx_chain, sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Rel Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynFlow.Explorer import full_rel_explorer\n",
    "\n",
    "triples = full_rel_explorer(\n",
    "    corpus_folder=corpus_folder,\n",
    "    pattern=pattern,            # or leave None to use default\n",
    "    target_lemma=target_lemma,\n",
    "    target_pos=target_pos,\n",
    "    rel=\"chi_case & chi_det & pa_obl\",\n",
    "    # rel=\"chi_aux & chi_nsubj & chi_obj & chi_punct\",\n",
    "    # rel=\"chi_discourse > chi_punct & chi_punct\",\n",
    "    mode = 'close', # 'open', 'close', 'closeh'\n",
    ")\n",
    "\n",
    "print(len(triples))\n",
    "\n",
    "# inspect a few\n",
    "for fname, sent, found_paths_details_list in triples[:10]:\n",
    "    # found_paths_details_list is a list of (ctx_nodes, path_str) tuples\n",
    "    for ctx_nodes, path_str in found_paths_details_list:\n",
    "        # ctx_nodes is a list of \"lemma/pos\", join using ' > ' to print\n",
    "        ctx_chain = \" > \".join(ctx_nodes)\n",
    "        print(f\"{fname:20} | {path_str:15} | {ctx_chain:20} | {sent}\")\n",
    "\n",
    "# Save to csv\n",
    "import csv\n",
    "out_path = f'{output_explorer}/full_rel_explorer.csv'\n",
    "with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    w = csv.writer(f, delimiter='\\t')\n",
    "    w.writerow([\"file\", \"path\", \"ctx_nodes\", \"sentence\"])\n",
    "    for fname, sent, ctx_nodes, path in rel_explorer_results:\n",
    "        ctx_chain = \" > \".join(ctx_nodes)\n",
    "        w.writerow([fname, path, ctx_chain, sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/volt/bach/SynFlow/output/air-N-1750-1799/Explorer/air_N_arg_comb_1_hops.csv', sep='&')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from SynFlow.Explorer import trim_and_merge\n",
    "df_file = '/home/volt/bach/SynFlow/output/air-N-1750-1799/Explorer/air_N_arg_comb_1_hops.csv'\n",
    "trimmed_rels = ['chi_punct', 'chi_det', 'pa_parataxis','chi_discourse']\n",
    "trim_and_merge(df_file=df_file, trimmed_rels=trimmed_rels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialisations Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynFlow.Explorer.trimming import spe_group\n",
    "\n",
    "df_path = '/home/volt/bach/SynFlow/output/air-N-1750-1799/Explorer/air_N_arg_comb_1_hops_trimmed.csv'\n",
    "tree = spe_group(df_path, output_folder=output_explorer, target_lemma=target_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Slot df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote slot‐fillers to /home/volt/bach/SynFlow/output/air-N-1750-1799/Embedding/air_samples_slotdf_all.csv (6156 rows), dropped 5695 tokens.\n"
     ]
    }
   ],
   "source": [
    "from SynFlow.Explorer import build_slot_df\n",
    "\n",
    "df_slots = build_slot_df(\n",
    "    corpus_folder=corpus_folder,\n",
    "    template='[chi_amod]', # Example: '[chi_nsubj][chi_obj][chi_obl > chi_case]'\n",
    "    target_lemma=target_lemma,\n",
    "    target_pos=target_pos,\n",
    "    pattern=pattern,\n",
    "    freq_path='/home/volt/bach/pilot_data/RSC/lemma_pos_init_freq.txt', # Be sure that the freq_path matches that of the filter format\n",
    "    freq_min=1,\n",
    "    freq_max=100_000_000,\n",
    "    filtered_pos=[],\n",
    "    filler_format='lemma/pos', # lemma/deprel or 'lemma/pos'\n",
    "    output_folder= output_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from the general slots DataFrame\n",
    "from SynFlow.Explorer import sample_slot_df\n",
    "n = 5\n",
    "df_sample = sample_slot_df(\n",
    "    input_csv=f\"{output_embedding}/{target_lemma}_samples_slotdf_all.csv\",\n",
    "    output_csv=f\"{output_embedding}/{target_lemma}_samples_slotdf_{n}.csv\",\n",
    "    n=n,\n",
    "    seed=42,\n",
    "    mode= 'NA'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template='[chi_nsubj][chi_obj][chi_obl > chi_case]'\n",
    "# slots     = template.strip(\"[]\").split(\"][\")\n",
    "# print(slots)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bython311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
