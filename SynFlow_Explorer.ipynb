{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Stanza parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "# stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse') # initialize English neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [\n",
    "        \"hell yeah!\"\n",
    "        #  'The boss runs the company.',\n",
    "        #  'The company is run by the boss', \n",
    "        #  'The company is run in the dark.', \n",
    "        #  'He runs in the jungle.',\n",
    "        #  'The roads run through the city.',\n",
    "        #  'He runs his finger through his hair.',\n",
    "        #  'The computer runs fast.',\n",
    "        #  'The car runs really fast.'\n",
    "         ]\n",
    "\n",
    "# sents = ['MISS NORMAN : Will you do me the honour to meet me at the bridgehead at half-past nine practically at once ?']\n",
    "target = 'miss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sents:\n",
    "    doc = nlp(sent) # run annotation over a sentence\n",
    "    print('sentence:', sent)\n",
    "    # print(doc)\n",
    "    # print(doc.entities)\n",
    "    print(*[f'word: {word.text}\\tlemma: {word.lemma}\\tpos: {word.pos}\\tid: {word.id}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "    print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from collections import defaultdict\n",
    "\n",
    "sents = ['The boss runs the company.',\n",
    "         'The company is run by the boss', \n",
    "         'The company is run in the dark.', \n",
    "         'He runs in the jungle.',\n",
    "         'The roads run through the city.',\n",
    "         'He runs his finger through his hair.',\n",
    "         'The computer runs fast.',\n",
    "         'The car runs really fast.']\n",
    "target = 'run'\n",
    "\n",
    "# 1. load the English pipeline (tokeniser-POS-lemma-dependency)\n",
    "nlp = stanza.Pipeline(\n",
    "        'en', processors='tokenize,pos,lemma,depparse',\n",
    "        tokenize_no_ssplit=True,  # treat each string as a single sentence\n",
    "        verbose=False)\n",
    "\n",
    "results = []               # (sent_id, dep_lemma, deprel)\n",
    "for sent_id, text in enumerate(sents, 1):\n",
    "    doc = nlp(text)\n",
    "    sent = doc.sentences[0]               # exactly one per string\n",
    "    for w in sent.words:                  # iterate over tokens/words\n",
    "        if w.lemma == target:             # <- our target lemma\n",
    "            head_id = w.id\n",
    "            # collect *immediate* dependents of this “run”\n",
    "            for d in sent.words:\n",
    "                if d.head == head_id:\n",
    "                    results.append((sent_id, d.lemma, d.deprel))\n",
    "\n",
    "# pretty-print\n",
    "for sent_id, lem, rel in results:\n",
    "    print(f'S{sent_id}: {lem:<10}  {rel}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents = ['The boss runs the company.',\n",
    "#          'The company is run by the boss', \n",
    "#          'The company is run in the dark.', \n",
    "#          'He runs in the jungle.',\n",
    "#          'The roads run through the city.',\n",
    "#          'He runs his finger through his hair.',\n",
    "#          'The computer runs fast.',\n",
    "#          'The car runs really fast.'\n",
    "#          ]\n",
    "\n",
    "# sents = [\n",
    "#     \"Freedom is priceless.\",\n",
    "#     \"She fought for freedom during the revolution.\",\n",
    "#     \"The court finally granted him the freedom to speak openly.\",\n",
    "#     \"Within the classroom, freedom of thought nurtures creativity.\",\n",
    "#     \"The towering bronze sculpture, Freedom, dominates the plaza.\",\n",
    "#     \"After the last exam, the students burst outside in pure freedom.\",\n",
    "#     \"Digital tracking can quietly erode freedom online.\",\n",
    "#     \"We debated whether freedom or security mattered more.\",\n",
    "#     \"Without self-control, freedom often collapses into chaos.\",\n",
    "#     \"He inhaled deeply, freedom flooding his lungs at the prison gates.\"\n",
    "# ]\n",
    "\n",
    "# sents = [\n",
    "#     \"The table shook during the earthquake.\",\n",
    "#     \"She carved her initials into the wooden table.\",\n",
    "#     \"After dinner, they sat around the table and talked for hours.\",\n",
    "#     \"The architect presented a glass table as the room's centerpiece.\",\n",
    "#     \"Please table the motion until next week’s meeting.\",\n",
    "#     \"We sorted the data into a table for easier comparison.\",\n",
    "#     \"The cat leapt onto the table, knocking over a vase.\",\n",
    "#     \"Negotiators agreed to table further discussion until sunrise.\",\n",
    "#     \"Beneath the table, a hidden drawer contained old photographs.\",\n",
    "#     \"A picnic table stood alone under the oak tree.\"\n",
    "# ]\n",
    "\n",
    "sents = [\n",
    "    \"This article is interesting.\",\n",
    "    \"An interesting twist changed the plot completely.\",\n",
    "    \"He found the lecture interesting despite the late hour.\",\n",
    "    \"Someone interesting moved into the apartment next door.\",\n",
    "    \"The most interesting of the artifacts was the jade mask.\",\n",
    "    \"Keep your questions interesting and concise.\",\n",
    "    \"They made the workshop interesting by adding hands-on demos.\",\n",
    "    \"What I find interesting is how quickly trends shift.\",\n",
    "    \"Do you have anything interesting to read on the train?\",\n",
    "    \"Interesting, she thought, how silence can speak louder than words.\"\n",
    "]\n",
    "\n",
    "\n",
    "TARGET = 'interesting' \n",
    "\n",
    "MAX_DEPTH  = 2             # you can pass (1,), (2,), (1,2,3) …\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "nlp = stanza.Pipeline(\n",
    "        \"en\",\n",
    "        processors=\"tokenize,pos,lemma,depparse\",\n",
    "        tokenize_no_ssplit=True,\n",
    "        verbose=False)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "def collect_connected(sent, target_lemma, max_depth):\n",
    "    \"\"\"\n",
    "    Return {depth: [(lemma, path)]} where 'path' is a string like\n",
    "    '↓obj' or '↑nsubj:pass > ↓obl' showing the route from the target\n",
    "    to the node.  Traversal is undirected, up to max_depth edges.\n",
    "    \"\"\"\n",
    "    id2word   = {w.id: w for w in sent.words}\n",
    "    neighbours = defaultdict(list)                  # id -> [(word, label)]\n",
    "\n",
    "    # build bidirectional edges\n",
    "    for w in sent.words:\n",
    "        if w.head == 0:                             # ROOT has no parent\n",
    "            continue\n",
    "        head = id2word[w.head]\n",
    "        neighbours[w.id].append((head, f\"↑{w.deprel}\"))   # child -> parent\n",
    "        neighbours[head.id].append((w, f\"↓{w.deprel}\"))   # parent -> child\n",
    "\n",
    "    result = defaultdict(list)                      # depth -> [(lemma, path)]\n",
    "    for w in sent.words:\n",
    "        if w.lemma != target_lemma:\n",
    "            continue                                # other lemmas not our start\n",
    "        q = deque([(w, 0, [])])                     # node, depth, path so far\n",
    "        visited = {w.id}\n",
    "        while q:\n",
    "            node, d, path = q.popleft()\n",
    "            if d == max_depth:                      # stop expanding beyond limit\n",
    "                continue\n",
    "            for nb, rel in neighbours[node.id]:\n",
    "                if nb.id in visited:\n",
    "                    continue\n",
    "                nd     = d + 1\n",
    "                npath  = path + [rel]\n",
    "                result[nd].append((nb.lemma, \" > \".join(npath)))\n",
    "                visited.add(nb.id)\n",
    "                q.append((nb, nd, npath))\n",
    "    return result\n",
    "# ------------------------------------------------------------------ #\n",
    "\n",
    "all_hits = defaultdict(lambda: defaultdict(list))   # sent_id -> depth -> items\n",
    "for sid, text in enumerate(sents, 1):\n",
    "    sent = nlp(text).sentences[0]\n",
    "    dep_map = collect_connected(sent, TARGET, MAX_DEPTH)\n",
    "    for d, items in dep_map.items():\n",
    "        all_hits[sid][d].extend(items)\n",
    "\n",
    "# --- demo print ---------------------------------------------------- #\n",
    "for sid in sorted(all_hits):\n",
    "    print(f\"\\nSentence {sid}: {sents[sid-1]}\")\n",
    "    for d in sorted(all_hits[sid]):\n",
    "        print(f\"  depth {d}:\")\n",
    "        for lem, rel_path in all_hits[sid][d]:\n",
    "            print(f\"      {lem:<10}  {rel_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "pattern = re.compile(\n",
    "    r'([^\\t]+)\\t'      # word form\n",
    "    r'([^\\t]+)\\t'      # lemma\n",
    "    r'([^\\t])[^\\t]*\\t' # POS (UPOS or XPOS)\n",
    "    r'([^\\t]+)\\t'      # ID\n",
    "    r'([^\\t]+)\\t'      # HEAD\n",
    "    r'([^\\t]+)'        # DEPREL\n",
    ")\n",
    "\n",
    "target_lemma = 'air'\n",
    "target_pos = 'N'\n",
    "\n",
    "# # All\n",
    "period = '1750-1799'\n",
    "corpus_folder = f'/home/volt/bach/pilot_data/RSC/1750-1799_che'\n",
    "output_folder = f'/home/volt/bach/SynFlow/output/{target_lemma}-{target_pos}-{period}'\n",
    "output_explorer = f'{output_folder}/Explorer'\n",
    "output_embedding = f'{output_folder}/Embedding'\n",
    "\n",
    "# Decades\n",
    "# period = '1790'\n",
    "# corpus_folder = f'/home/volt/bach/pilot_data/RSC/1750-1799_che_decades/{period}'\n",
    "# output_folder = f'/home/volt/bach/SynFlow/output/{target_lemma}-{target_pos}-{period}'\n",
    "# visualisation_folder = f'/home/volt/bach/SynFlow/visualisation/{target_lemma}-{target_pos}-{period}'\n",
    "\n",
    "# Half decades\n",
    "# period = '1770-1774'\n",
    "# corpus_folder = f'/home/volt/bach/pilot_data/RSC/1750-1799_che_half_decades/{period}'\n",
    "# output_folder = f'/home/volt/bach/SynFlow/output/{target_lemma}-{target_pos}-{period}'\n",
    "# output_explorer = f'{output_folder}/Explorer'\n",
    "# output_embedding = f'{output_folder}/Embedding'\n",
    "\n",
    "if not os.path.exists(output_explorer):\n",
    "    os.makedirs(output_explorer)\n",
    "\n",
    "if not os.path.exists(output_embedding):\n",
    "    os.makedirs(output_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the distribution of different syntactic relationships from the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynFlow.Explorer import arg_explorer\n",
    "dist = arg_explorer(\n",
    "    corpus_folder=corpus_folder,\n",
    "    target_lemma=target_lemma,\n",
    "    target_pos=target_pos,\n",
    "    max_length=1,\n",
    "    top_n=30,\n",
    "    pattern=pattern,\n",
    "    output_folder=output_explorer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Argument Combination Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 unique full-pattern string for 1 token\n",
    "from SynFlow.Explorer import arg_comb_explorer\n",
    "\n",
    "ctr = arg_comb_explorer(\n",
    "    corpus_folder=corpus_folder,\n",
    "    target_lemma=target_lemma,\n",
    "    target_pos=target_pos,\n",
    "    max_length=1,\n",
    "    top_n=30,\n",
    "    output_folder=output_explorer,\n",
    "    pattern=pattern\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rel Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynFlow.Explorer import rel_explorer\n",
    "\n",
    "triples = rel_explorer(\n",
    "    corpus_folder=corpus_folder,\n",
    "    pattern=pattern,            # or leave None to use default\n",
    "    target_lemma=target_lemma,\n",
    "    target_pos=target_pos,\n",
    "    rel=\"chi_nmod\",\n",
    ")\n",
    "\n",
    "# inspect a few\n",
    "# for sent, ctx_list, path in triples[:10]:\n",
    "#     print(f\"{path:>15}  {' > '.join(ctx_list):20} | {sent}\")\n",
    "\n",
    "for fname, sent, ctx_list, path in triples[:100]:\n",
    "    # ctx_list là một list các \"lemma/pos\", nối bằng ' > ' để in cho dễ nhìn\n",
    "    ctx_chain = \" > \".join(ctx_list)\n",
    "    print(f\"{fname:20} | {path:15} | {ctx_chain:20} | {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Rel Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynFlow.Explorer import full_rel_explorer\n",
    "\n",
    "triples = full_rel_explorer(\n",
    "    corpus_folder=corpus_folder,\n",
    "    pattern=pattern,            # or leave None to use default\n",
    "    target_lemma=target_lemma,\n",
    "    target_pos=target_pos,\n",
    "    rel=\"chi_case\",\n",
    "    # rel=\"chi_aux & chi_nsubj & chi_obj & chi_punct\",\n",
    "    # rel=\"chi_discourse > chi_punct & chi_punct\",\n",
    "    mode = 'open', # 'open', 'close', 'closeh'\n",
    ")\n",
    "\n",
    "print(len(triples))\n",
    "\n",
    "# inspect a few\n",
    "for fname, sent, found_paths_details_list in triples[:10]:\n",
    "    # found_paths_details_list is a list of (ctx_nodes, path_str) tuples\n",
    "    for ctx_nodes, path_str in found_paths_details_list:\n",
    "        # ctx_nodes is a list of \"lemma/pos\", join using ' > ' to print\n",
    "        ctx_chain = \" > \".join(ctx_nodes)\n",
    "        print(f\"{fname:20} | {path_str:15} | {ctx_chain:20} | {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/volt/bach/SynFlow/output/air-N-1770-1774/Explorer/air_arg_comb_1_hops.csv', sep='&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from SynFlow.Explorer import trim_and_merge\n",
    "df_file = '/home/volt/bach/SynFlow/output/air-N-1770-1774/Explorer/air_arg_comb_1_hops.csv'\n",
    "trimmed_rels = ['chi_punct', 'chi_det', 'pa_parataxis','chi_discourse']\n",
    "trim_and_merge(df_file=df_file, trimmed_rels=trimmed_rels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialisations Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynFlow.Explorer.trimming import spe_group\n",
    "\n",
    "df_path = '/home/volt/bach/SynFlow/output/air-N-1770-1774/Explorer/air_arg_comb_1_hops_trimmed.csv'\n",
    "tree = spe_group(df_path, output_folder=output_explorer, target_lemma=target_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Slot df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynFlow.Explorer import build_slot_df\n",
    "\n",
    "df_slots = build_slot_df(\n",
    "    corpus_folder=corpus_folder,\n",
    "    template='[chi_compound]',\n",
    "    target_lemma=target_lemma,\n",
    "    target_pos=target_pos,\n",
    "    pattern=pattern,\n",
    "    freq_path='/home/volt/bach/pilot_data/RSC/lemma_pos_init_freq.txt',\n",
    "    freq_min=1,\n",
    "    freq_max=100_000_000,\n",
    "    filtered_pos=[],\n",
    "    filler_format='lemma/pos', # lemma/deprel or 'lemma/pos'\n",
    "    output_folder= output_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from the general slots DataFrame\n",
    "from SynFlow.Explorer import sample_slot_df\n",
    "\n",
    "df_sample = sample_slot_df(\n",
    "    input_csv=f\"{output_embedding}/{target_lemma}_samples_all_slots.csv\",\n",
    "    output_csv=f\"{output_embedding}/{target_lemma}_samples_{n}_slots.csv\",\n",
    "    n=n,\n",
    "    seed=42,\n",
    "    mode= 'NA'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template='[chi_nsubj][chi_obj][chi_obl > chi_case]'\n",
    "# slots     = template.strip(\"[]\").split(\"][\")\n",
    "# print(slots)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bython311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
